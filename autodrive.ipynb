{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a361ff9d",
   "metadata": {},
   "source": [
    "## Install and Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b404eb9",
   "metadata": {},
   "source": [
    "**Use CUDA 12.4 with these torch versions!!!**\n",
    "\n",
    "- [Download CUDA 12.4](https://developer.nvidia.com/cuda-12-4-0-download-archive?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local)\n",
    "- [Download cuDNN 8.9.7](https://developer.nvidia.com/rdp/cudnn-archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44560faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "\n",
    "#%pip install ultralytics\n",
    "#%pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "#%pip install opencv-python\n",
    "#%pip install cvzone\n",
    "#%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7514e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "import cvzone\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f8948",
   "metadata": {},
   "source": [
    "## Configuration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80e6dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "VIDEO_SOURCE = 0\n",
    "#VIDEO_SOURCE = \"data/test-videos/test_video.mp4\"\n",
    "MODEL_PEOPLE_CARS_PATH = \"yolo-weights/yolo11l.pt\"\n",
    "MODEL_TRAFFIC_LIGHTS_PATH = \"trained-models/yolo_large_trafficlights.pt\"\n",
    "MODEL_LANE_SEGMENTATION_PATH = \"trained-models/yolo_large_lane.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46ef6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Class Names ---\n",
    "\n",
    "CLASS_NAMES_BASE = [\"pedestrian\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "                    \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "                    \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\",\n",
    "                    \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\n",
    "                    \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
    "                    \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "                    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
    "                    \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n",
    "                    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "                    \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\",\n",
    "                    \"toothbrush\"]\n",
    "\n",
    "CLASS_NAMES_TRAFFIC = [\"green-lights\", \"red-lights\", \"yellow-lights\"]\n",
    "\n",
    "CLASS_NAMES_LANES = ['lane', 'road']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization Settings ---\n",
    "COLOR_PEOPLE_CARS = (255, 178, 50)   # Light Blue\n",
    "COLOR_TRAFFIC_LIGHTS = (0, 255, 0)  # Green\n",
    "COLOR_LANES = (0, 0, 255)         # Red\n",
    "LANE_OVERLAY_ALPHA = 0.4            # Transparency of the lane\n",
    "\n",
    "CONFIDENCE_THRESHOLD_DETECTION = 0.3 # Minimum confidence for detection boxes\n",
    "CONFIDENCE_THRESHOLD_SEGMENTATION = 0.4 # Minimum confidence for segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "909e82c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda [NVIDIA GeForce GTX 1660 SUPER]\n"
     ]
    }
   ],
   "source": [
    "# --- GPU Setup ---\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device} [{torch.cuda.get_device_name(0)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125538db",
   "metadata": {},
   "source": [
    "## Model and Source Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c70ef2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Models ---\n",
    "print(\"Loading models...\")\n",
    "model_people_cars = YOLO(MODEL_PEOPLE_CARS_PATH)\n",
    "model_people_cars.to(device)\n",
    "\n",
    "model_traffic_lights = YOLO(MODEL_TRAFFIC_LIGHTS_PATH)\n",
    "model_traffic_lights.to(device)\n",
    "\n",
    "model_lane_segmentation = YOLO(MODEL_LANE_SEGMENTATION_PATH)\n",
    "model_lane_segmentation.to(device)\n",
    "print(\"Models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccfb33c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening video source: Camera\n"
     ]
    }
   ],
   "source": [
    "# --- Video Capture ---\n",
    "print(f\"Opening video source: {\"Camera\" if VIDEO_SOURCE == 0 else VIDEO_SOURCE}\")\n",
    "cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video source\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57fe55a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video resolution: 640x480\n"
     ]
    }
   ],
   "source": [
    "# --- Get Frame Dimensions ---\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f\"Video resolution: {frame_width}x{frame_height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b6d7a",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    # Read a frame from the video source\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video stream or error reading frame.\")\n",
    "        break\n",
    "\n",
    "    # --- Inference ---\n",
    "    results_people_cars = model_people_cars.predict(frame, device=device, verbose=False, conf=CONFIDENCE_THRESHOLD_DETECTION)\n",
    "    results_traffic_lights = model_traffic_lights.predict(frame, device=device, verbose=False, conf=CONFIDENCE_THRESHOLD_DETECTION)\n",
    "    results_lanes = model_lane_segmentation.predict(frame, device=device, verbose=False, conf=CONFIDENCE_THRESHOLD_SEGMENTATION)\n",
    "\n",
    "    processed_frame = frame.copy()\n",
    "    segmentation_overlay = np.zeros_like(processed_frame, dtype=np.uint8)\n",
    "\n",
    "    # --- Model 1: Process Detections for Pedestrians and Vehicles ---\n",
    "    if results_people_cars and len(results_people_cars) > 0:\n",
    "        r = results_people_cars[0]\n",
    "        boxes = r.boxes.cpu().numpy()\n",
    "\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            conf = box.conf[0]\n",
    "            cls_id = int(box.cls[0])\n",
    "\n",
    "            obj_class = CLASS_NAMES_BASE[cls_id]\n",
    "\n",
    "            if obj_class in [\"pedestrian\", \"car\", \"truck\", \"bus\", \"motorbike\", \"train\"]:\n",
    "                label = f\"{obj_class}: {conf:.2f}\"\n",
    "                cvzone.cornerRect(processed_frame, (x1, y1, w, h), l=9, t=2, rt=2, colorR=COLOR_PEOPLE_CARS, colorC=COLOR_PEOPLE_CARS)\n",
    "                cvzone.putTextRect(processed_frame, label, (max(0, x1), max(35, y1 - 10)), scale=0.6, thickness=1, offset=3, colorR=COLOR_PEOPLE_CARS, colorT=(255, 255, 255))\n",
    "\n",
    "    # --- Model 2: Process Detections for Traffic Lights ---\n",
    "    if results_traffic_lights and len(results_traffic_lights) > 0:\n",
    "        r = results_traffic_lights[0]\n",
    "        boxes = r.boxes.cpu().numpy()\n",
    "\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            conf = box.conf[0]\n",
    "            cls_id = int(box.cls[0])\n",
    "\n",
    "            obj_class = CLASS_NAMES_TRAFFIC[cls_id]\n",
    "            label = f\"{obj_class}: {conf:.2f}\"\n",
    "            cvzone.cornerRect(processed_frame, (x1, y1, w, h), l=9, t=2, rt=2, colorR=COLOR_TRAFFIC_LIGHTS, colorC=COLOR_TRAFFIC_LIGHTS)\n",
    "            cvzone.putTextRect(processed_frame, label, (max(0, x1), max(35, y1 - 10)), scale=0.6, thickness=1, offset=3, colorR=COLOR_TRAFFIC_LIGHTS, colorT=(255, 255, 255))\n",
    "\n",
    "    # --- Model 3: Process Segmentation for Lanes ---\n",
    "    if results_lanes and len(results_lanes) > 0:\n",
    "        r = results_lanes[0]\n",
    "\n",
    "        if r.masks is not None:\n",
    "            masks_data = r.masks.data # contains the mask tensors [N, H, W]\n",
    "\n",
    "            # Check if any masks were detected\n",
    "            if masks_data.shape[0] > 0:\n",
    "\n",
    "                combined_mask = torch.max(masks_data, dim=0)[0]\n",
    "\n",
    "                # Resize mask to original frame size using GPU acceleration\n",
    "                combined_mask_resized = torch.nn.functional.interpolate(\n",
    "                    combined_mask.unsqueeze(0).unsqueeze(0),\n",
    "                    size=(frame_height, frame_width), \n",
    "                    mode='bilinear',\n",
    "                    align_corners=False\n",
    "                ).squeeze()\n",
    "\n",
    "                # Get a binary mask (0 or 1)\n",
    "                binary_mask = (combined_mask_resized > CONFIDENCE_THRESHOLD_SEGMENTATION).float()\n",
    "\n",
    "                # --- Create Colored Overlay ---\n",
    "                binary_mask_cpu = binary_mask.cpu().numpy().astype(np.uint8)\n",
    "                bool_mask_cpu = binary_mask_cpu.astype(bool)\n",
    "                segmentation_overlay[bool_mask_cpu] = COLOR_LANES\n",
    "\n",
    "                # --- Blend the Overlay with the Frame ---\n",
    "                processed_frame = cv2.addWeighted(\n",
    "                    segmentation_overlay,\n",
    "                    LANE_OVERLAY_ALPHA,\n",
    "                    processed_frame,\n",
    "                    1 - LANE_OVERLAY_ALPHA,\n",
    "                    0\n",
    "                )\n",
    "\n",
    "    # --- Display ---\n",
    "    cv2.imshow(\"Combined YOLO Output\", processed_frame)\n",
    "\n",
    "    # --- Exit Condition ---\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# --- Cleanup ---\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
